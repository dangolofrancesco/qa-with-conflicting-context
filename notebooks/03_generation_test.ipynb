{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a5924aa",
   "metadata": {},
   "source": [
    "# Task 3: Model Inference & Test Generation\n",
    "\n",
    "Generate predictions from both fine-tuned models on the test set.\n",
    "\n",
    "**Models:**\n",
    "- Model A (Context-Only): `fdangolo/flan-t5-context-only`\n",
    "- Model B (Explain-and-Answer): `fdangolo/flan-t5-exp-ans`\n",
    "\n",
    "**Test Data:** ConflictQA test set (896 examples)\n",
    "\n",
    "**Expected Runtime:** 15-30 minutes on Colab GPU\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Enable GPU runtime (Runtime > Change runtime type > T4 GPU)\n",
    "2. Upload `test_context_only.jsonl` to Google Drive\n",
    "3. Have your Hugging Face token ready for authentication\n",
    "4. Execute cells in order from top to bottom\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60a0b4",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive & Setup Paths\n",
    "\n",
    "Mount Drive to access the test data and save predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70240c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_PATH = \"/content/drive/MyDrive/reproducing_project\"\n",
    "DATA_PATH = f\"{BASE_PATH}/data/splits\"\n",
    "\n",
    "print(f\"Drive mounted\")\n",
    "print(f\"Base path: {BASE_PATH}\")\n",
    "print(f\"Data path: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1dde9a",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies\n",
    "\n",
    "Install required libraries for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab8c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft accelerate bitsandbytes torch\n",
    "!pip install -q huggingface_hub tqdm\n",
    "\n",
    "print(\"All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba0afb",
   "metadata": {},
   "source": [
    "## Step 3: Authenticate with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4361a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c763fca",
   "metadata": {},
   "source": [
    "## Step 4: Load Test Dataset\n",
    "\n",
    "Load the test set from `test_context_only.jsonl`. Both models use the same input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "test_file_path = f\"{DATA_PATH}/test_context_only.jsonl\"\n",
    "\n",
    "print(f\"Loading test data from: {test_file_path}\")\n",
    "\n",
    "if not os.path.exists(test_file_path):\n",
    "    print(f\"Error: File not found at {test_file_path}\")\n",
    "    print(f\"Please ensure test_context_only.jsonl is uploaded to: {DATA_PATH}/\")\n",
    "else:\n",
    "    test_dataset = load_dataset('json', data_files={'test': test_file_path})['test']\n",
    "    \n",
    "    print(\"\\nExample Test Input:\")\n",
    "    print(test_dataset[0]['input'][:300] + \"...\")\n",
    "    print(f\"\\nLoaded {len(test_dataset)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95427e",
   "metadata": {},
   "source": [
    "## Step 5: Generate Predictions - Model A (Context-Only)\n",
    "\n",
    "This model answers questions directly without providing explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af654a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_model_id = \"google/flan-t5-base\"\n",
    "adapter_A_id = \"fdangolo/flan-t5-context-only\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"\\nLoading base model: {base_model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_model_id).to(device)\n",
    "\n",
    "print(f\"Loading LoRA adapter: {adapter_A_id}\")\n",
    "model = PeftModel.from_pretrained(model, adapter_A_id).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model A loaded successfully\\n\")\n",
    "\n",
    "predictions_A = []\n",
    "print(f\"Generating predictions for Model A...\")\n",
    "print(f\"Processing {len(test_dataset)} examples...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in tqdm(test_dataset, desc=\"Model A\"):\n",
    "        inputs = tokenizer(example['input'], return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=50,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        prediction_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions_A.append(prediction_text)\n",
    "\n",
    "print(f\"\\nGenerated {len(predictions_A)} predictions\")\n",
    "\n",
    "output_file_A = f\"{BASE_PATH}/predictions_A.json\"\n",
    "with open(output_file_A, 'w') as f:\n",
    "    json.dump(predictions_A, f, indent=2)\n",
    "\n",
    "print(f\"Saved predictions to: {output_file_A}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Sample Predictions (Model A)\")\n",
    "print(\"=\"*70)\n",
    "for i in range(min(3, len(test_dataset))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Input: {test_dataset[i]['input'][:200]}...\")\n",
    "    print(f\"Prediction: {predictions_A[i]}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a4039",
   "metadata": {},
   "source": [
    "## Step 6: Generate Predictions - Model B (Explain-and-Answer)\n",
    "\n",
    "This model explains the conflict between sources before providing an answer.\n",
    "\n",
    "The output format is: Explanation + \"\\n\\n\" + Answer\n",
    "\n",
    "We parse the output to extract only the final answer for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f69c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared\\n\")\n",
    "\n",
    "adapter_B_id = \"fdangolo/flan-t5-exp-ans\"\n",
    "\n",
    "print(f\"Loading base model: {base_model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_model_id).to(device)\n",
    "\n",
    "print(f\"Loading LoRA adapter: {adapter_B_id}\")\n",
    "model = PeftModel.from_pretrained(model, adapter_B_id).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model B loaded successfully\\n\")\n",
    "\n",
    "predictions_B_raw = []\n",
    "predictions_B_parsed = []\n",
    "print(f\"Generating predictions for Model B...\")\n",
    "print(f\"Processing {len(test_dataset)} examples...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in tqdm(test_dataset, desc=\"Model B\"):\n",
    "        inputs = tokenizer(example['input'], return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=300,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        raw_prediction_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions_B_raw.append(raw_prediction_text)\n",
    "        \n",
    "        # Parse output: split by \"\\n\\n\" and take the last part (the answer)\n",
    "        parsed_answer = raw_prediction_text.split(\"\\n\\n\")[-1].strip()\n",
    "        predictions_B_parsed.append(parsed_answer)\n",
    "\n",
    "print(f\"\\nGenerated {len(predictions_B_parsed)} predictions\")\n",
    "\n",
    "output_file_B_raw = f\"{BASE_PATH}/predictions_B_raw.json\"\n",
    "with open(output_file_B_raw, 'w') as f:\n",
    "    json.dump(predictions_B_raw, f, indent=2)\n",
    "print(f\"Saved raw predictions to: {output_file_B_raw}\")\n",
    "\n",
    "output_file_B = f\"{BASE_PATH}/predictions_B.json\"\n",
    "with open(output_file_B, 'w') as f:\n",
    "    json.dump(predictions_B_parsed, f, indent=2)\n",
    "print(f\"Saved parsed predictions to: {output_file_B}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Sample Predictions (Model B)\")\n",
    "print(\"=\"*70)\n",
    "for i in range(min(3, len(test_dataset))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Input: {test_dataset[i]['input'][:200]}...\")\n",
    "    print(f\"\\nRaw Output (Explanation + Answer):\")\n",
    "    print(f\"  {predictions_B_raw[i]}\")\n",
    "    print(f\"\\nParsed Answer Only:\")\n",
    "    print(f\"  {predictions_B_parsed[i]}\")\n",
    "    print(\"-\" * 70)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad21b2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Summary\n",
    "\n",
    "Generation complete for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTION GENERATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Test examples processed: {len(test_dataset)}\")\n",
    "print(f\"  Model A predictions: {len(predictions_A)}\")\n",
    "print(f\"  Model B predictions: {len(predictions_B_parsed)}\")\n",
    "\n",
    "print(f\"\\nOutput files (saved to Google Drive):\")\n",
    "print(f\"  {BASE_PATH}/predictions_A.json\")\n",
    "print(f\"  {BASE_PATH}/predictions_B.json\")\n",
    "print(f\"  {BASE_PATH}/predictions_B_raw.json (with explanations)\")\n",
    "\n",
    "print(f\"\\nFile sizes:\")\n",
    "import os\n",
    "if os.path.exists(f\"{BASE_PATH}/predictions_A.json\"):\n",
    "    size_A = os.path.getsize(f\"{BASE_PATH}/predictions_A.json\") / 1024\n",
    "    print(f\"  predictions_A.json: {size_A:.1f} KB\")\n",
    "if os.path.exists(f\"{BASE_PATH}/predictions_B.json\"):\n",
    "    size_B = os.path.getsize(f\"{BASE_PATH}/predictions_B.json\") / 1024\n",
    "    print(f\"  predictions_B.json: {size_B:.1f} KB\")\n",
    "if os.path.exists(f\"{BASE_PATH}/predictions_B_raw.json\"):\n",
    "    size_B_raw = os.path.getsize(f\"{BASE_PATH}/predictions_B_raw.json\") / 1024\n",
    "    print(f\"  predictions_B_raw.json: {size_B_raw:.1f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Download prediction files from Google Drive\")\n",
    "print(\"  2. Run evaluation script: evaluate_conflictqa.py\")\n",
    "print(\"  3. Compare results with original paper metrics\")\n",
    "print(\"  4. Document findings in REPRODUCIBILITY_LOG.md\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\nQuick statistics:\")\n",
    "print(f\"  Model A - Average prediction length: {sum(len(p.split()) for p in predictions_A) / len(predictions_A):.1f} words\")\n",
    "print(f\"  Model B - Average prediction length: {sum(len(p.split()) for p in predictions_B_parsed) / len(predictions_B_parsed):.1f} words\")\n",
    "print(f\"  Model B - Average raw output length: {sum(len(p.split()) for p in predictions_B_raw) / len(predictions_B_raw):.1f} words\")\n",
    "\n",
    "print(\"\\nAll predictions generated successfully\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d199dc76",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "### About the Predictions\n",
    "\n",
    "**Model A (Context-Only)**\n",
    "- Generates short, direct answers\n",
    "- No explanation provided\n",
    "- Optimized for concise responses\n",
    "\n",
    "**Model B (Explain-and-Answer)**\n",
    "- Generates explanation first, then answer\n",
    "- Output format: Explanation\\n\\nAnswer\n",
    "- Both raw (with explanation) and parsed (answer only) versions are saved\n",
    "- The parsed version is used for evaluation\n",
    "\n",
    "### File Locations\n",
    "\n",
    "Prediction files saved to Google Drive:\n",
    "```\n",
    "{BASE_PATH}/\n",
    "├── predictions_A.json          # Model A: Direct answers\n",
    "├── predictions_B.json          # Model B: Parsed answers (for evaluation)\n",
    "└── predictions_B_raw.json      # Model B: Full output (with explanations)\n",
    "```\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "To evaluate these predictions:\n",
    "1. Download the prediction files from Google Drive\n",
    "2. Place them in the src/evaluation/ directory\n",
    "3. Run the evaluation script\n",
    "\n",
    "### GPU Usage\n",
    "\n",
    "- Inference time: 15-30 minutes on Colab T4 GPU\n",
    "- Memory: Both models fit comfortably in T4's 15GB memory\n",
    "- Cost: Free tier is sufficient\n",
    "\n",
    "---\n",
    "\n",
    "For reproducibility research:\n",
    "- Document inference time in REPRODUCIBILITY_LOG.md\n",
    "- Compare generated samples with original paper examples\n",
    "- Note any differences in model behavior"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
