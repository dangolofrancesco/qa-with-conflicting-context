{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90727e5c",
   "metadata": {},
   "source": [
    "# Task 4: Model Evaluation\n",
    "\n",
    "Evaluate the predictions from both fine-tuned models using the ConflictQA test set.\n",
    "\n",
    "This notebook computes Exact Match (EM) and F1 scores for:\n",
    "- Overall performance (Total)\n",
    "- Conflicting questions (C)\n",
    "- Non-conflicting questions (NC)\n",
    "\n",
    "The evaluation logic follows the original evaluate_conflictqa.py script.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Enable GPU runtime (optional for evaluation)\n",
    "2. Upload required files to Google Drive:\n",
    "   - ConflictQA_Dataset.json (ground truth)\n",
    "   - predictions_A.json (Model A predictions)\n",
    "   - predictions_B.json (Model B predictions)\n",
    "3. Execute cells in order\n",
    "\n",
    "Expected runtime: Less than 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526286c7",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection & Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab1c204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Local (VS Code)\n",
      "Using local repository path\n",
      "Base path: /Users/francescodangolo/Desktop/CS 421 - Natural Language Processing/Research Project/qa-with-conflicting-context\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"Environment: Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Environment: Local (VS Code)\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_PATH = \"/content/drive/MyDrive/reproducing_project\"\n",
    "    print(f\"Drive mounted successfully\")\n",
    "else:\n",
    "    BASE_PATH = os.path.abspath(\"..\")\n",
    "    print(f\"Using local repository path\")\n",
    "\n",
    "print(f\"Base path: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2a803",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e61adc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies installed successfully\n"
     ]
    }
   ],
   "source": [
    "!pip install -q numpy pandas\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "print(\"Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bd0c8",
   "metadata": {},
   "source": [
    "## Step 3: Define Evaluation Functions\n",
    "\n",
    "These functions are adapted from the original evaluate_conflictqa.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b718ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"Normalize text for comparison: lowercase, remove punctuation and articles.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, ground_truth):\n",
    "    \"\"\"Check if normalized prediction exactly matches normalized ground truth.\"\"\"\n",
    "    return int(normalize_text(prediction) == normalize_text(ground_truth))\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    \"\"\"Compute F1 score between prediction and ground truth tokens.\"\"\"\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(ground_truth).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    num_common = len(common_tokens)\n",
    "    \n",
    "    if num_common == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a878687",
   "metadata": {},
   "source": [
    "## Step 4: Load Ground Truth and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d54579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 813 test examples\n",
      "Loaded 813 predictions for Model A\n",
      "Loaded 813 predictions for Model B\n",
      "\n",
      "Validation passed: All datasets have 813 examples\n"
     ]
    }
   ],
   "source": [
    "dataset_path = f\"{BASE_PATH}/data/ConflictQA_Dataset.json\"\n",
    "predictions_a_path = f\"{BASE_PATH}/predictions_A.json\"\n",
    "predictions_b_path = f\"{BASE_PATH}/predictions_B.json\"\n",
    "\n",
    "try:\n",
    "    full_df = pd.read_json(dataset_path)\n",
    "    ground_truth_df = full_df[full_df['split'] == 'test'].reset_index(drop=True)\n",
    "    print(f\"Loaded {len(ground_truth_df)} test examples\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset not found at {dataset_path}\")\n",
    "\n",
    "try:\n",
    "    with open(predictions_a_path, 'r') as f:\n",
    "        predictions_A = json.load(f)\n",
    "    print(f\"Loaded {len(predictions_A)} predictions for Model A\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: predictions_A.json not found at {predictions_a_path}\")\n",
    "\n",
    "try:\n",
    "    with open(predictions_b_path, 'r') as f:\n",
    "        predictions_B = json.load(f)\n",
    "    print(f\"Loaded {len(predictions_B)} predictions for Model B\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: predictions_B.json not found at {predictions_b_path}\")\n",
    "\n",
    "assert len(ground_truth_df) == len(predictions_A) == len(predictions_B), \"Data length mismatch\"\n",
    "print(f\"\\nValidation passed: All datasets have {len(ground_truth_df)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef2d9f",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "575ae1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model A (Context-Only)...\n",
      "Model A Results: {'EM-T': np.float64(47.23), 'F1-T': np.float64(58.93), 'EM-C': np.float64(42.03), 'F1-C': np.float64(53.41), 'EM-NC': np.float64(49.01), 'F1-NC': np.float64(60.81)}\n",
      "\n",
      "Evaluating Model B (Explain-and-Answer)...\n",
      "Model A Results: {'EM-T': np.float64(47.23), 'F1-T': np.float64(58.93), 'EM-C': np.float64(42.03), 'F1-C': np.float64(53.41), 'EM-NC': np.float64(49.01), 'F1-NC': np.float64(60.81)}\n",
      "\n",
      "Evaluating Model B (Explain-and-Answer)...\n",
      "Model B Results: {'EM-T': np.float64(0.62), 'F1-T': np.float64(13.72), 'EM-C': np.float64(0.0), 'F1-C': np.float64(12.19), 'EM-NC': np.float64(0.83), 'F1-NC': np.float64(14.24)}\n",
      "Model B Results: {'EM-T': np.float64(0.62), 'F1-T': np.float64(13.72), 'EM-C': np.float64(0.0), 'F1-C': np.float64(12.19), 'EM-NC': np.float64(0.83), 'F1-NC': np.float64(14.24)}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(predictions, ground_truth_df):\n",
    "    \"\"\"\n",
    "    Evaluate predictions against ground truth using EM and F1 metrics.\n",
    "    Returns scores for Total, Conflict, and Non-Conflict subsets.\n",
    "    \"\"\"\n",
    "    em_total = []\n",
    "    f1_total = []\n",
    "    em_c = []\n",
    "    em_nc = []\n",
    "    f1_c = []\n",
    "    f1_nc = []\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        pred = predictions[i]\n",
    "        ground_truth_list = ground_truth_df.iloc[i][\"ambigqa_answer\"]\n",
    "        is_conflict = ground_truth_df.iloc[i][\"secondAnswerExist\"] == \"A\"\n",
    "\n",
    "        max_em = max(compute_exact_match(pred, str(ref)) for ref in ground_truth_list)\n",
    "        max_f1 = max(compute_f1(pred, str(ref)) for ref in ground_truth_list)\n",
    "\n",
    "        em_total.append(max_em)\n",
    "        f1_total.append(max_f1)\n",
    "        \n",
    "        if is_conflict:\n",
    "            em_c.append(max_em)\n",
    "            f1_c.append(max_f1)\n",
    "        else:\n",
    "            em_nc.append(max_em)\n",
    "            f1_nc.append(max_f1)\n",
    "\n",
    "    return {\n",
    "        \"EM-T\": np.round(np.mean(em_total) * 100, 2),\n",
    "        \"F1-T\": np.round(np.mean(f1_total) * 100, 2),\n",
    "        \"EM-C\": np.round(np.mean(em_c) * 100, 2),\n",
    "        \"F1-C\": np.round(np.mean(f1_c) * 100, 2),\n",
    "        \"EM-NC\": np.round(np.mean(em_nc) * 100, 2),\n",
    "        \"F1-NC\": np.round(np.mean(f1_nc) * 100, 2),\n",
    "    }\n",
    "\n",
    "print(\"Evaluating Model A (Context-Only)...\")\n",
    "results_A = evaluate_model(predictions_A, ground_truth_df)\n",
    "print(f\"Model A Results: {results_A}\")\n",
    "\n",
    "print(\"\\nEvaluating Model B (Explain-and-Answer)...\")\n",
    "results_B = evaluate_model(predictions_B, ground_truth_df)\n",
    "print(f\"Model B Results: {results_B}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba8ed6",
   "metadata": {},
   "source": [
    "## Step 6: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7e22000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL EVALUATION RESULTS\n",
      "==================================================\n",
      "\n",
      "                               EM-T   F1-T   EM-C   F1-C  EM-NC  F1-NC\n",
      "Model A (Context-Only)        47.23  58.93  42.03  53.41  49.01  60.81\n",
      "Model B (Explain-and-Answer)   0.62  13.72   0.00  12.19   0.83  14.24\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(\n",
    "    [results_A, results_B], \n",
    "    index=[\"Model A (Context-Only)\", \"Model B (Explain-and-Answer)\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "print(results_df.to_string())\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc72c1f",
   "metadata": {},
   "source": [
    "## Step 7: Update Reproducibility Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "915b9349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated /Users/francescodangolo/Desktop/CS 421 - Natural Language Processing/Research Project/qa-with-conflicting-context/REPRODUCIBILITY_LOG.md\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "log_path = os.path.join(BASE_PATH, \"REPRODUCIBILITY_LOG.md\")\n",
    "\n",
    "log_entry = f\"\"\"\n",
    "---\n",
    "**Evaluation Completed - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}**\n",
    "\n",
    "### Model A: Context-Only (fdangolo/flan-t5-context-only)\n",
    "- EM-Total: {results_A['EM-T']}%\n",
    "- F1-Total: {results_A['F1-T']}%\n",
    "- EM-Conflict: {results_A['EM-C']}%\n",
    "- F1-Conflict: {results_A['F1-C']}%\n",
    "- EM-Non-Conflict: {results_A['EM-NC']}%\n",
    "- F1-Non-Conflict: {results_A['F1-NC']}%\n",
    "\n",
    "### Model B: Explain-and-Answer (fdangolo/flan-t5-exp-ans)\n",
    "- EM-Total: {results_B['EM-T']}%\n",
    "- F1-Total: {results_B['F1-T']}%\n",
    "- EM-Conflict: {results_B['EM-C']}%\n",
    "- F1-Conflict: {results_B['F1-C']}%\n",
    "- EM-Non-Conflict: {results_B['EM-NC']}%\n",
    "- F1-Non-Conflict: {results_B['F1-NC']}%\n",
    "\n",
    "Environment: {'Google Colab' if IN_COLAB else 'Local (VS Code)'}\n",
    "Evaluated on: {len(ground_truth_df)} test examples\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with open(log_path, 'a') as f:\n",
    "        f.write(log_entry)\n",
    "    print(f\"Successfully updated {log_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error updating log: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61ee66c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING DIAGNOSTIC ---\n",
      "Loading base model and adapter for Model B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescodangolo/Desktop/CS 421 - Natural Language Processing/Research Project/qa-with-conflicting-context/venv/lib/python3.11/site-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['corda_config', 'qalora_group_size', 'target_parameters', 'trainable_token_indices', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 samples...\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescodangolo/Desktop/CS 421 - Natural Language Processing/Research Project/qa-with-conflicting-context/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SAMPLE 1 ---\n",
      "INPUT:\n",
      "question: Who created the convention on the rights of the child?\n",
      "\n",
      "contexts: The Convention on the Rights of the Child was adopted by the General Assembly of the United Nations by its resolution 44/25 of 20 November 1989.\n",
      "---\n",
      "Nov 17, 2014 ... The Convention on the Rights of the Child is an internatio...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "most common\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "The General Assembly of the United Nations\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "The General Assembly of the United Nations\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 2 ---\n",
      "INPUT:\n",
      "question: Who played zoe hart on hart of dixie?\n",
      "\n",
      "contexts: Jan 18, 2019 ... Dr. Zoe Hart-Kinsella is the protagonist and titular character. She is a girl with a plan. That plan is focused around her dedication to ...\n",
      "---\n",
      "Aug 25, 2022 ... Rachel Bilson (Zoe Hart) · Jaime King (Lemon Breeland) · Cress...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "Rachel Bilson\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "Only few context contain the correct answers. Others are not Rachel Bilson\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "Only few context contain the correct answers. Others are not Rachel Bilson\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 2 ---\n",
      "INPUT:\n",
      "question: Who played zoe hart on hart of dixie?\n",
      "\n",
      "contexts: Jan 18, 2019 ... Dr. Zoe Hart-Kinsella is the protagonist and titular character. She is a girl with a plan. That plan is focused around her dedication to ...\n",
      "---\n",
      "Aug 25, 2022 ... Rachel Bilson (Zoe Hart) · Jaime King (Lemon Breeland) · Cress...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "Rachel Bilson\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "Only few context contain the correct answers. Others are not Rachel Bilson\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "Only few context contain the correct answers. Others are not Rachel Bilson\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 3 ---\n",
      "INPUT:\n",
      "question: Mainland greece is a body of land with water on three sides called?\n",
      "\n",
      "contexts: Greece is a ___ land framed by water. mountainous. The mainland is a ______, a body of land with water on three sides. peninsula. Many ancient Greeks lived ...\n",
      "---\n",
      "A piece of land surrounded by water on three si...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "peninsula\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "I chose it because it is the only correct answer and it is explicitly mentioned in context 1 and 2 and it is also mentioned in context 3 and 4 and it is also mentioned in context 5 and 6 and it is also mentioned in context 7 and 8 and it is also mentioned in context 9 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is mentioned in context 11 and 10 and\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "I chose it because it is the only correct answer and it is explicitly mentioned in context 1 and 2 and it is also mentioned in context 3 and 4 and it is also mentioned in context 5 and 6 and it is also mentioned in context 7 and 8 and it is also mentioned in context 9 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is mentioned in context 11 and 10 and\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 3 ---\n",
      "INPUT:\n",
      "question: Mainland greece is a body of land with water on three sides called?\n",
      "\n",
      "contexts: Greece is a ___ land framed by water. mountainous. The mainland is a ______, a body of land with water on three sides. peninsula. Many ancient Greeks lived ...\n",
      "---\n",
      "A piece of land surrounded by water on three si...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "peninsula\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "I chose it because it is the only correct answer and it is explicitly mentioned in context 1 and 2 and it is also mentioned in context 3 and 4 and it is also mentioned in context 5 and 6 and it is also mentioned in context 7 and 8 and it is also mentioned in context 9 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is mentioned in context 11 and 10 and\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "I chose it because it is the only correct answer and it is explicitly mentioned in context 1 and 2 and it is also mentioned in context 3 and 4 and it is also mentioned in context 5 and 6 and it is also mentioned in context 7 and 8 and it is also mentioned in context 9 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is also mentioned in context 11 and 10 and it is mentioned in context 11 and 10 and\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 4 ---\n",
      "INPUT:\n",
      "question: What does the msc in msc cruises stand for?\n",
      "\n",
      "contexts: In 1988, Mediterranean Shipping Company (MSC) entered the cruise business by buying the liner Monterey. In 1989, MSC bought Lauro Lines. The new company was ...\n",
      "---\n",
      "Jun 18, 2021 ... MSC stands for Mediterranean Shipping Company, which ...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "Mediterranean Shipping Company\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "I chose it because it is the only correct answer and it is explicitly mentioned in context 1 and 2 Mediterranean Shipping Company\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "I chose it because it is the only correct answer and it is explicitly mentioned in context 1 and 2 Mediterranean Shipping Company\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 4 ---\n",
      "INPUT:\n",
      "question: What does the msc in msc cruises stand for?\n",
      "\n",
      "contexts: In 1988, Mediterranean Shipping Company (MSC) entered the cruise business by buying the liner Monterey. In 1989, MSC bought Lauro Lines. The new company was ...\n",
      "---\n",
      "Jun 18, 2021 ... MSC stands for Mediterranean Shipping Company, which ...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "Mediterranean Shipping Company\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "I chose it because it is the only correct answer and it is explicitly mentioned in context 1 and 2 Mediterranean Shipping Company\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "I chose it because it is the only correct answer and it is explicitly mentioned in context 1 and 2 Mediterranean Shipping Company\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 5 ---\n",
      "INPUT:\n",
      "question: The boiling point of water is 100 degrees celsius express this in si units?\n",
      "\n",
      "contexts: The boiling point of water is 100°C to express it in SI unit i.e. Kelvin we should add 273 to 0°C Kelvin = °C + 273. = 100+273.\n",
      "---\n",
      "Jun 17, 2017 ... Boiling point of water is 100 degree celsius to expres...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "373 degree Kelvin\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "Only few context contain the correct answer. Others are not. Temp. on Celsius scale = Temp. on Celsius scale + 273 Therefore, 100 + 273 = 373 K\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "Only few context contain the correct answer. Others are not. Temp. on Celsius scale = Temp. on Celsius scale + 273 Therefore, 100 + 273 = 373 K\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 5 ---\n",
      "INPUT:\n",
      "question: The boiling point of water is 100 degrees celsius express this in si units?\n",
      "\n",
      "contexts: The boiling point of water is 100°C to express it in SI unit i.e. Kelvin we should add 273 to 0°C Kelvin = °C + 273. = 100+273.\n",
      "---\n",
      "Jun 17, 2017 ... Boiling point of water is 100 degree celsius to expres...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "373 degree Kelvin\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "Only few context contain the correct answer. Others are not. Temp. on Celsius scale = Temp. on Celsius scale + 273 Therefore, 100 + 273 = 373 K\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "Only few context contain the correct answer. Others are not. Temp. on Celsius scale = Temp. on Celsius scale + 273 Therefore, 100 + 273 = 373 K\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 6 ---\n",
      "INPUT:\n",
      "question: Who wrote trust and believe by keyshia cole?\n",
      "\n",
      "contexts: \"Trust and Believe\" is a song by R&B singer/songwriter Keyshia Cole. It serves as the second single from her fifth studio album, Woman to Woman and the ...\n",
      "---\n",
      "The song was written with Jessy Wilson and Guordan Banks [AKA The Righterz...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "Cole,Camper, Banks,J. Wilson\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "Only few context contain the correct answers. Others are not. Guordan Banks\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "Only few context contain the correct answers. Others are not. Guordan Banks\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 6 ---\n",
      "INPUT:\n",
      "question: Who wrote trust and believe by keyshia cole?\n",
      "\n",
      "contexts: \"Trust and Believe\" is a song by R&B singer/songwriter Keyshia Cole. It serves as the second single from her fifth studio album, Woman to Woman and the ...\n",
      "---\n",
      "The song was written with Jessy Wilson and Guordan Banks [AKA The Righterz...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "Cole,Camper, Banks,J. Wilson\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "Only few context contain the correct answers. Others are not. Guordan Banks\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "Only few context contain the correct answers. Others are not. Guordan Banks\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 7 ---\n",
      "INPUT:\n",
      "question: The era of the great mughals began with the accession of?\n",
      "\n",
      "contexts: Before making a major political decision, the Ottoman sultan was expected to. seek a fatwa. ... The Era of the Great Mughals began with the accession of.\n",
      "---\n",
      "Mughal dynasty, Muslim dynasty of Turkic-Mongol origin that rul...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "1580 to 1650\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "I chose it because it is the only correct answer and it is explicitly mentioned in context 1 and it is the only correct answer. Others are not Babur's\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "I chose it because it is the only correct answer and it is explicitly mentioned in context 1 and it is the only correct answer. Others are not Babur's\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 7 ---\n",
      "INPUT:\n",
      "question: The era of the great mughals began with the accession of?\n",
      "\n",
      "contexts: Before making a major political decision, the Ottoman sultan was expected to. seek a fatwa. ... The Era of the Great Mughals began with the accession of.\n",
      "---\n",
      "Mughal dynasty, Muslim dynasty of Turkic-Mongol origin that rul...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "1580 to 1650\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "I chose it because it is the only correct answer and it is explicitly mentioned in context 1 and it is the only correct answer. Others are not Babur's\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "I chose it because it is the only correct answer and it is explicitly mentioned in context 1 and it is the only correct answer. Others are not Babur's\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 8 ---\n",
      "INPUT:\n",
      "question: In photosynthesis the carbon in co2 is initially fixed to what molecule?\n",
      "\n",
      "contexts: ... a radioactively-labeled carbon dioxide molecule to evaluate photosynthetic ... In photosynthesis, the carbon in CO2 is initially fixed to what molecule?\n",
      "---\n",
      "Mar 18, 2020 ... However, two key restriction...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "RuBP\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "Only few context contain the correct answers. Others are not RuBP\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "Only few context contain the correct answers. Others are not RuBP\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 8 ---\n",
      "INPUT:\n",
      "question: In photosynthesis the carbon in co2 is initially fixed to what molecule?\n",
      "\n",
      "contexts: ... a radioactively-labeled carbon dioxide molecule to evaluate photosynthetic ... In photosynthesis, the carbon in CO2 is initially fixed to what molecule?\n",
      "---\n",
      "Mar 18, 2020 ... However, two key restriction...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "RuBP\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "Only few context contain the correct answers. Others are not RuBP\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "Only few context contain the correct answers. Others are not RuBP\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 9 ---\n",
      "INPUT:\n",
      "question: Who sings the whiskey ain't workin anymore?\n",
      "\n",
      "contexts: Oct 3, 2011 ... Travis Tritt - The Whiskey Ain't Workin' (It's All About To Change) · Comments239.\n",
      "---\n",
      "\"The Whiskey Ain't Workin'\" is a song recorded by American country music artists Travis Tritt and Marty Stuart. It was released in N...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "Travis Tritt\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "Only few context contain the correct answers. Others are not. Travis Tritt and Marty Stuart\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "Only few context contain the correct answers. Others are not. Travis Tritt and Marty Stuart\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 9 ---\n",
      "INPUT:\n",
      "question: Who sings the whiskey ain't workin anymore?\n",
      "\n",
      "contexts: Oct 3, 2011 ... Travis Tritt - The Whiskey Ain't Workin' (It's All About To Change) · Comments239.\n",
      "---\n",
      "\"The Whiskey Ain't Workin'\" is a song recorded by American country music artists Travis Tritt and Marty Stuart. It was released in N...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "Travis Tritt\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "Only few context contain the correct answers. Others are not. Travis Tritt and Marty Stuart\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "Only few context contain the correct answers. Others are not. Travis Tritt and Marty Stuart\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 10 ---\n",
      "INPUT:\n",
      "question: Where does the spinal cord become cauda equina?\n",
      "\n",
      "contexts: The cauda equina is a group of nerves and nerve roots stemming from the distal end of the spinal cord, typically levels L1-L5 and contains axons of nerves ...\n",
      "---\n",
      "The spinal cord ends around L1, consequently, the caudal nerve roots...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "L1\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "The answer contains the correct answer. Others are not. L1-L5\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "The answer contains the correct answer. Others are not. L1-L5\n",
      "========================================\n",
      "\n",
      "--- SAMPLE 10 ---\n",
      "INPUT:\n",
      "question: Where does the spinal cord become cauda equina?\n",
      "\n",
      "contexts: The cauda equina is a group of nerves and nerve roots stemming from the distal end of the spinal cord, typically levels L1-L5 and contains axons of nerves ...\n",
      "---\n",
      "The spinal cord ends around L1, consequently, the caudal nerve roots...\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "L1\n",
      "\n",
      "MODEL B RAW OUTPUT:\n",
      "The answer contains the correct answer. Others are not. L1-L5\n",
      "\n",
      "PARSED ANSWER (what we evaluated):\n",
      "The answer contains the correct answer. Others are not. L1-L5\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Configuration ---\n",
    "base_model_id = \"google/flan-t5-base\" #\n",
    "adapter_B_id = \"fdangolo/flan-t5-exp-ans\" #\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"--- STARTING DIAGNOSTIC ---\")\n",
    "print(f\"Loading base model and adapter for Model B...\")\n",
    "\n",
    "# --- Load Model & Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_model_id).to(device)\n",
    "model = PeftModel.from_pretrained(model, adapter_B_id).to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Load Test Data ---\n",
    "# We need the 'input' and the 'output' (ground truth)\n",
    "data_path = os.path.join(BASE_PATH, \"data/splits/test_context_only.jsonl\")\n",
    "ground_truth_dataset = load_dataset('json', data_files={'test': data_path})['test']\n",
    "test_inputs = ground_truth_dataset['input']\n",
    "test_truths = ground_truth_dataset['output']\n",
    "\n",
    "print(f\"Generating 10 samples...\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# --- Generate 10 Samples for Manual Inspection ---\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        input_text = test_inputs[i]\n",
    "        ground_truth = test_truths[i]\n",
    "        \n",
    "        # Prepare input\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "        \n",
    "        # Generate output (raw text)\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=300, \n",
    "            early_stopping=True\n",
    "        )\n",
    "        raw_prediction_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Apply our parser\n",
    "        parsed_answer = raw_prediction_text.split(\"\\n\\n\")[-1].strip()\n",
    "        \n",
    "        # --- Print Comparison ---\n",
    "        print(f\"\\n--- SAMPLE {i+1} ---\")\n",
    "        print(f\"INPUT:\\n{input_text[:300]}...\\n\")\n",
    "        print(f\"GROUND TRUTH ANSWER:\\n{ground_truth}\\n\")\n",
    "        print(f\"MODEL B RAW OUTPUT:\\n{raw_prediction_text}\\n\")\n",
    "        print(f\"PARSED ANSWER (what we evaluated):\\n{parsed_answer}\")\n",
    "        print(\"=\"*40)\n",
    "\n",
    "# Clean up memory\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
