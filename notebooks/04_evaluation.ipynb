{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90727e5c",
   "metadata": {},
   "source": [
    "# Task 4: Model Evaluation\n",
    "\n",
    "Evaluate the predictions from both fine-tuned models using the ConflictQA test set.\n",
    "\n",
    "This notebook computes Exact Match (EM) and F1 scores for:\n",
    "- Overall performance (Total)\n",
    "- Conflicting questions (C)\n",
    "- Non-conflicting questions (NC)\n",
    "\n",
    "The evaluation logic follows the original evaluate_conflictqa.py script.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Enable GPU runtime (optional for evaluation)\n",
    "2. Upload required files to Google Drive:\n",
    "   - ConflictQA_Dataset.json (ground truth)\n",
    "   - predictions_A.json (Model A predictions)\n",
    "   - predictions_B.json (Model B predictions)\n",
    "3. Execute cells in order\n",
    "\n",
    "Expected runtime: Less than 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526286c7",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive & Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab1c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_PATH = \"/content/drive/MyDrive/reproducing_project\"\n",
    "\n",
    "print(f\"Drive mounted\")\n",
    "print(f\"Base path: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2a803",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61adc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy pandas\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "print(\"Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bd0c8",
   "metadata": {},
   "source": [
    "## Step 3: Define Evaluation Functions\n",
    "\n",
    "These functions are adapted from the original evaluate_conflictqa.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b718ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"Normalize text for comparison: lowercase, remove punctuation and articles.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, ground_truth):\n",
    "    \"\"\"Check if normalized prediction exactly matches normalized ground truth.\"\"\"\n",
    "    return int(normalize_text(prediction) == normalize_text(ground_truth))\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    \"\"\"Compute F1 score between prediction and ground truth tokens.\"\"\"\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(ground_truth).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    num_common = len(common_tokens)\n",
    "    \n",
    "    if num_common == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a878687",
   "metadata": {},
   "source": [
    "## Step 4: Load Ground Truth and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d54579",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = f\"{BASE_PATH}/data/ConflictQA_Dataset.json\"\n",
    "predictions_a_path = f\"{BASE_PATH}/predictions_A.json\"\n",
    "predictions_b_path = f\"{BASE_PATH}/predictions_B.json\"\n",
    "\n",
    "try:\n",
    "    full_df = pd.read_json(dataset_path)\n",
    "    ground_truth_df = full_df[full_df['split'] == 'test'].reset_index(drop=True)\n",
    "    print(f\"Loaded {len(ground_truth_df)} test examples\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset not found at {dataset_path}\")\n",
    "\n",
    "try:\n",
    "    with open(predictions_a_path, 'r') as f:\n",
    "        predictions_A = json.load(f)\n",
    "    print(f\"Loaded {len(predictions_A)} predictions for Model A\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: predictions_A.json not found at {predictions_a_path}\")\n",
    "\n",
    "try:\n",
    "    with open(predictions_b_path, 'r') as f:\n",
    "        predictions_B = json.load(f)\n",
    "    print(f\"Loaded {len(predictions_B)} predictions for Model B\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: predictions_B.json not found at {predictions_b_path}\")\n",
    "\n",
    "assert len(ground_truth_df) == len(predictions_A) == len(predictions_B), \"Data length mismatch\"\n",
    "print(f\"\\nValidation passed: All datasets have {len(ground_truth_df)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef2d9f",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ae1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, ground_truth_df):\n",
    "    \"\"\"\n",
    "    Evaluate predictions against ground truth using EM and F1 metrics.\n",
    "    Returns scores for Total, Conflict, and Non-Conflict subsets.\n",
    "    \"\"\"\n",
    "    em_total = []\n",
    "    f1_total = []\n",
    "    em_c = []\n",
    "    em_nc = []\n",
    "    f1_c = []\n",
    "    f1_nc = []\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        pred = predictions[i]\n",
    "        ground_truth_list = ground_truth_df.iloc[i][\"ambigqa_answer\"]\n",
    "        is_conflict = ground_truth_df.iloc[i][\"secondAnswerExist\"] == \"A\"\n",
    "\n",
    "        max_em = max(compute_exact_match(pred, str(ref)) for ref in ground_truth_list)\n",
    "        max_f1 = max(compute_f1(pred, str(ref)) for ref in ground_truth_list)\n",
    "\n",
    "        em_total.append(max_em)\n",
    "        f1_total.append(max_f1)\n",
    "        \n",
    "        if is_conflict:\n",
    "            em_c.append(max_em)\n",
    "            f1_c.append(max_f1)\n",
    "        else:\n",
    "            em_nc.append(max_em)\n",
    "            f1_nc.append(max_f1)\n",
    "\n",
    "    return {\n",
    "        \"EM-T\": np.round(np.mean(em_total) * 100, 2),\n",
    "        \"F1-T\": np.round(np.mean(f1_total) * 100, 2),\n",
    "        \"EM-C\": np.round(np.mean(em_c) * 100, 2),\n",
    "        \"F1-C\": np.round(np.mean(f1_c) * 100, 2),\n",
    "        \"EM-NC\": np.round(np.mean(em_nc) * 100, 2),\n",
    "        \"F1-NC\": np.round(np.mean(f1_nc) * 100, 2),\n",
    "    }\n",
    "\n",
    "print(\"Evaluating Model A (Context-Only)...\")\n",
    "results_A = evaluate_model(predictions_A, ground_truth_df)\n",
    "print(f\"Model A Results: {results_A}\")\n",
    "\n",
    "print(\"\\nEvaluating Model B (Explain-and-Answer)...\")\n",
    "results_B = evaluate_model(predictions_B, ground_truth_df)\n",
    "print(f\"Model B Results: {results_B}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba8ed6",
   "metadata": {},
   "source": [
    "## Step 6: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e22000",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(\n",
    "    [results_A, results_B], \n",
    "    index=[\"Model A (Context-Only)\", \"Model B (Explain-and-Answer)\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "print(results_df.to_string())\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
