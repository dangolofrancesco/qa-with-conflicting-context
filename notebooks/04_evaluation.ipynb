{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90727e5c",
   "metadata": {},
   "source": [
    "# Task 4: Model Evaluation\n",
    "\n",
    "Evaluate the predictions from both fine-tuned models using the ConflictQA test set.\n",
    "\n",
    "This notebook computes Exact Match (EM) and F1 scores for:\n",
    "- Overall performance (Total)\n",
    "- Conflicting questions (C)\n",
    "- Non-conflicting questions (NC)\n",
    "\n",
    "The evaluation logic follows the original evaluate_conflictqa.py script.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Enable GPU runtime (optional for evaluation)\n",
    "2. Upload required files to Google Drive:\n",
    "   - ConflictQA_Dataset.json (ground truth)\n",
    "   - predictions_A.json (Model A predictions)\n",
    "   - predictions_B.json (Model B predictions)\n",
    "3. Execute cells in order\n",
    "\n",
    "Expected runtime: Less than 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526286c7",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection & Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab1c204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Local (VS Code)\n",
      "Using local repository path\n",
      "Base path: /Users/francescodangolo/Desktop/CS 421 - Natural Language Processing/Research Project/qa-with-conflicting-context\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"Environment: Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Environment: Local (VS Code)\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_PATH = \"/content/drive/MyDrive/reproducing_project\"\n",
    "    print(f\"Drive mounted successfully\")\n",
    "else:\n",
    "    BASE_PATH = os.path.abspath(\"..\")\n",
    "    print(f\"Using local repository path\")\n",
    "\n",
    "print(f\"Base path: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2a803",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e61adc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies installed successfully\n"
     ]
    }
   ],
   "source": [
    "!pip install -q numpy pandas\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "print(\"Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bd0c8",
   "metadata": {},
   "source": [
    "## Step 3: Define Evaluation Functions\n",
    "\n",
    "These functions are adapted from the original evaluate_conflictqa.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b718ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"Normalize text for comparison: lowercase, remove punctuation and articles.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, ground_truth):\n",
    "    \"\"\"Check if normalized prediction exactly matches normalized ground truth.\"\"\"\n",
    "    return int(normalize_text(prediction) == normalize_text(ground_truth))\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    \"\"\"Compute F1 score between prediction and ground truth tokens.\"\"\"\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(ground_truth).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    num_common = len(common_tokens)\n",
    "    \n",
    "    if num_common == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a878687",
   "metadata": {},
   "source": [
    "## Step 4: Load Ground Truth and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d54579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 813 test examples\n",
      "Loaded 813 predictions for Model A\n",
      "Loaded 813 predictions for Model B\n",
      "\n",
      "Validation passed: All datasets have 813 examples\n"
     ]
    }
   ],
   "source": [
    "dataset_path = f\"{BASE_PATH}/data/ConflictQA_Dataset.json\"\n",
    "predictions_a_path = f\"{BASE_PATH}/predictions_A.json\"\n",
    "predictions_b_path = f\"{BASE_PATH}/predictions_B.json\"\n",
    "\n",
    "try:\n",
    "    full_df = pd.read_json(dataset_path)\n",
    "    ground_truth_df = full_df[full_df['split'] == 'test'].reset_index(drop=True)\n",
    "    print(f\"Loaded {len(ground_truth_df)} test examples\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset not found at {dataset_path}\")\n",
    "\n",
    "try:\n",
    "    with open(predictions_a_path, 'r') as f:\n",
    "        predictions_A = json.load(f)\n",
    "    print(f\"Loaded {len(predictions_A)} predictions for Model A\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: predictions_A.json not found at {predictions_a_path}\")\n",
    "\n",
    "try:\n",
    "    with open(predictions_b_path, 'r') as f:\n",
    "        predictions_B = json.load(f)\n",
    "    print(f\"Loaded {len(predictions_B)} predictions for Model B\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: predictions_B.json not found at {predictions_b_path}\")\n",
    "\n",
    "assert len(ground_truth_df) == len(predictions_A) == len(predictions_B), \"Data length mismatch\"\n",
    "print(f\"\\nValidation passed: All datasets have {len(ground_truth_df)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef2d9f",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "575ae1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model A (Context-Only)...\n",
      "Model A Results: {'EM-T': np.float64(47.23), 'F1-T': np.float64(58.93), 'EM-C': np.float64(42.03), 'F1-C': np.float64(53.41), 'EM-NC': np.float64(49.01), 'F1-NC': np.float64(60.81)}\n",
      "\n",
      "Evaluating Model B (Explain-and-Answer)...\n",
      "Model A Results: {'EM-T': np.float64(47.23), 'F1-T': np.float64(58.93), 'EM-C': np.float64(42.03), 'F1-C': np.float64(53.41), 'EM-NC': np.float64(49.01), 'F1-NC': np.float64(60.81)}\n",
      "\n",
      "Evaluating Model B (Explain-and-Answer)...\n",
      "Model B Results: {'EM-T': np.float64(0.62), 'F1-T': np.float64(13.72), 'EM-C': np.float64(0.0), 'F1-C': np.float64(12.19), 'EM-NC': np.float64(0.83), 'F1-NC': np.float64(14.24)}\n",
      "Model B Results: {'EM-T': np.float64(0.62), 'F1-T': np.float64(13.72), 'EM-C': np.float64(0.0), 'F1-C': np.float64(12.19), 'EM-NC': np.float64(0.83), 'F1-NC': np.float64(14.24)}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(predictions, ground_truth_df):\n",
    "    \"\"\"\n",
    "    Evaluate predictions against ground truth using EM and F1 metrics.\n",
    "    Returns scores for Total, Conflict, and Non-Conflict subsets.\n",
    "    \"\"\"\n",
    "    em_total = []\n",
    "    f1_total = []\n",
    "    em_c = []\n",
    "    em_nc = []\n",
    "    f1_c = []\n",
    "    f1_nc = []\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        pred = predictions[i]\n",
    "        ground_truth_list = ground_truth_df.iloc[i][\"ambigqa_answer\"]\n",
    "        is_conflict = ground_truth_df.iloc[i][\"secondAnswerExist\"] == \"A\"\n",
    "\n",
    "        max_em = max(compute_exact_match(pred, str(ref)) for ref in ground_truth_list)\n",
    "        max_f1 = max(compute_f1(pred, str(ref)) for ref in ground_truth_list)\n",
    "\n",
    "        em_total.append(max_em)\n",
    "        f1_total.append(max_f1)\n",
    "        \n",
    "        if is_conflict:\n",
    "            em_c.append(max_em)\n",
    "            f1_c.append(max_f1)\n",
    "        else:\n",
    "            em_nc.append(max_em)\n",
    "            f1_nc.append(max_f1)\n",
    "\n",
    "    return {\n",
    "        \"EM-T\": np.round(np.mean(em_total) * 100, 2),\n",
    "        \"F1-T\": np.round(np.mean(f1_total) * 100, 2),\n",
    "        \"EM-C\": np.round(np.mean(em_c) * 100, 2),\n",
    "        \"F1-C\": np.round(np.mean(f1_c) * 100, 2),\n",
    "        \"EM-NC\": np.round(np.mean(em_nc) * 100, 2),\n",
    "        \"F1-NC\": np.round(np.mean(f1_nc) * 100, 2),\n",
    "    }\n",
    "\n",
    "print(\"Evaluating Model A (Context-Only)...\")\n",
    "results_A = evaluate_model(predictions_A, ground_truth_df)\n",
    "print(f\"Model A Results: {results_A}\")\n",
    "\n",
    "print(\"\\nEvaluating Model B (Explain-and-Answer)...\")\n",
    "results_B = evaluate_model(predictions_B, ground_truth_df)\n",
    "print(f\"Model B Results: {results_B}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba8ed6",
   "metadata": {},
   "source": [
    "## Step 6: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7e22000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL EVALUATION RESULTS\n",
      "==================================================\n",
      "\n",
      "                               EM-T   F1-T   EM-C   F1-C  EM-NC  F1-NC\n",
      "Model A (Context-Only)        47.23  58.93  42.03  53.41  49.01  60.81\n",
      "Model B (Explain-and-Answer)   0.62  13.72   0.00  12.19   0.83  14.24\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(\n",
    "    [results_A, results_B], \n",
    "    index=[\"Model A (Context-Only)\", \"Model B (Explain-and-Answer)\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "print(results_df.to_string())\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc72c1f",
   "metadata": {},
   "source": [
    "## Step 7: Update Reproducibility Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "915b9349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated /Users/francescodangolo/Desktop/CS 421 - Natural Language Processing/Research Project/qa-with-conflicting-context/REPRODUCIBILITY_LOG.md\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "log_path = os.path.join(BASE_PATH, \"REPRODUCIBILITY_LOG.md\")\n",
    "\n",
    "log_entry = f\"\"\"\n",
    "---\n",
    "**Evaluation Completed - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}**\n",
    "\n",
    "### Model A: Context-Only (fdangolo/flan-t5-context-only)\n",
    "- EM-Total: {results_A['EM-T']}%\n",
    "- F1-Total: {results_A['F1-T']}%\n",
    "- EM-Conflict: {results_A['EM-C']}%\n",
    "- F1-Conflict: {results_A['F1-C']}%\n",
    "- EM-Non-Conflict: {results_A['EM-NC']}%\n",
    "- F1-Non-Conflict: {results_A['F1-NC']}%\n",
    "\n",
    "### Model B: Explain-and-Answer (fdangolo/flan-t5-exp-ans)\n",
    "- EM-Total: {results_B['EM-T']}%\n",
    "- F1-Total: {results_B['F1-T']}%\n",
    "- EM-Conflict: {results_B['EM-C']}%\n",
    "- F1-Conflict: {results_B['F1-C']}%\n",
    "- EM-Non-Conflict: {results_B['EM-NC']}%\n",
    "- F1-Non-Conflict: {results_B['F1-NC']}%\n",
    "\n",
    "Environment: {'Google Colab' if IN_COLAB else 'Local (VS Code)'}\n",
    "Evaluated on: {len(ground_truth_df)} test examples\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with open(log_path, 'a') as f:\n",
    "        f.write(log_entry)\n",
    "    print(f\"Successfully updated {log_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error updating log: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
